# Multimodal Text Extraction Test Dataset

This repository contains a lightweight dataset of images and document scans designed for evaluating the text extraction capabilities of **multimodal large language models (LLMs)**. It includes synthetic and real-world examples commonly found in administrative forms and handwritten records.

## ðŸ“¦ Contents

- `images/` â€“ Sample images and scanned forms  
- `README.md` â€“ This file  
- `LICENSE` â€“ Usage license (Apache 2.0)

## ðŸ§ª Use Case

This dataset is intended for:
- Testing **OCR performance** of vision-language models  
- Benchmarking **handwriting transcription** and **form understanding**  
- Evaluating **zero-shot or few-shot capabilities** of small multimodal LLMs  
- Comparing model output to traditional OCR tools (e.g., EasyOCR, Tesseract)

## ðŸ“Š Evaluation Criteria

Text extraction accuracy can be measured using:
- Field-level precision (correct vs. incorrect extractions)
- Structural layout understanding
- Semantic recognition of labels and handwritten content

## ðŸ“„ License

This dataset is released under the **Apache 2.0** license. See the `LICENSE` file for more details.
